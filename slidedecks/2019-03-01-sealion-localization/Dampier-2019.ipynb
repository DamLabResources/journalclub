{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Identification and localization of Sea Lion anatomical features using transfer learning\n",
    "\n",
    "Will Dampier\n",
    "\n",
    "- In my occasionally free moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://player.vimeo.com/video/266222134\" width=\"640\" height=\"360\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<iframe src=\"https://player.vimeo.com/video/266222134\" width=\"640\" height=\"360\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Goal\n",
    "\n",
    "#### Turn this \n",
    "![unlabeled-lion](imgs/cutie2.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### To this\n",
    "![labeled-lion](imgs/cutie2_labeled.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## And eventually\n",
    "\n",
    "Are these two sea-lions the same?\n",
    "\n",
    "| Lion 1| Lion 2|\n",
    "|:------:|:-------:|\n",
    "|![unlabeled-lion](imgs/cutie2.JPG)| ![unlabeled-lion](imgs/cutie.JPG)|\n",
    "| | |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Previous Sea Lion identification research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![whisker labeling](imgs/marked_whiskers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![original requirements](imgs/orig_requirements.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But how do you get those 180 images in the wild?\n",
    "\n",
    "![Sleeping lion](imgs/one-side.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Biological Analogies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Neuron](imgs/neuron-field.png)\n",
    "\n",
    "[Source](http://neuroclusterbrain.com/neuron_model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Sleeping lion](imgs/Human_visual_pathway.svg)\n",
    "\n",
    "[Source](https://creativecommons.org/licenses/by-sa/4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![saccade example](imgs/saccades.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![saccade example](imgs/saccades-together.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computational Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Digital Cat](imgs/digital_cat.png)\n",
    "\n",
    "[Source](http://cs231n.github.io/classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![sliding convolutions](imgs/sliding_windows.gif)\n",
    "\n",
    "[Source](https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![sliding convolutions](imgs/sliding_windows_multi.gif)\n",
    "\n",
    "[Source](http://cs231n.github.io/convolutional-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "ResNet is the current state of the art model. It has 25.6M parameters to tune.\n",
    "![Resnet](imgs/resnet_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ImageNet & Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![ImageNet](imgs/imgnet-logo.jpg)\n",
    "\n",
    "\n",
    "15 millions labeled high-resolution images with around 22,000 categories. This collection was used to train ResNet to ~5% top-5 error rate.\n",
    "\n",
    "![imagenet examples](imgs/imagenet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Layers](imgs/feature_levels.png)\n",
    "\n",
    "[Source](https://torres.ai/artificial-intelligence-content/deeplearning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cool and all, but this is what we'd get. The ability to tell if the __whole image__ contained a sea lion.\n",
    "\n",
    "| Lion 97% | Lion 91% | Not Lion 0.12% |\n",
    "|:------:|:-------:|:-------:|\n",
    "|![unlabeled-lion](imgs/cutie2.JPG)| ![unlabeled-lion](imgs/cutie.JPG)| ![not-lion](imgs/not_lion.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Retina Net\n",
    "\n",
    "If you want to know __where__ an object is. You have to change your question  a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![yolo-1](imgs/yolo-1.jpeg)\n",
    "\n",
    "[Source](https://heartbeat.fritz.ai/gentle-guide-on-how-yolo-object-localization-works-with-keras-part-2-65fe59ac12d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![yolo-2](imgs/yolo-2.jpg)\n",
    "\n",
    "[Source](https://heartbeat.fritz.ai/gentle-guide-on-how-yolo-object-localization-works-with-keras-part-2-65fe59ac12d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Annotating Images\n",
    "\n",
    "![dataturks](imgs/dataturks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![annotation](imgs/annotation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Current Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Training was done on only 40 labeled images done by one annotator (me). I used the `mobilenet` architecture and the smallest model parameters possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![first-retinanet fun](imgs/first_retina_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![harder retinanet fun](imgs/hard_retina_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What's the image with the most \"recognizable\" face?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Not a face](imgs/not_a_face.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Future Directions\n",
    "\n",
    "(Condensed from [source](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![science](imgs/face_science.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Landmark detection\n",
    "\n",
    "![landmarks](imgs/face_mask.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### That's so 1990\n",
    "\n",
    "- Does not generalize well with faces that are not \"front facing\".\n",
    "- False positive rate increases as the database size increases.\n",
    "- Are these really the \"best\" points to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deep Learning Way\n",
    "\n",
    "![triplet](imgs/triplet_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This strategy has many advantages:\n",
    "  - Increasing the database __decreases__ false-positive rate.\n",
    "  - Images can be taken from any angle.\n",
    "  - Allows for \"outlier\" detection of novel faces that aren't in the database.\n",
    "  - Face databases can be constructed in linear time and searched in `log(n)` time.\n",
    "  - Generalizes to things beyond faces ... like whiskers and flippers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Challenges yet to overcome:\n",
    "  - We don't actually have the standard triplets. We have to make some assumptions about which sea lions are definitely not the same. \n",
    "    - Sea Lions recorded on disparate beaches on the same day are likely different lions.\n",
    "  - How do we deal with integrating flipper matching, face recognition, whisker recognition, etc into one score.\n",
    "  - Orders of magnitude fewer images. FaceNet is trained with at least 10 images from >50 million faces; we have ~3000 images of ? unique sea lions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
